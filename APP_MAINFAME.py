# GENAIV1_WATSONX.py ‚Äî Mainframe Modernization AI Assistant (IBM WATSONX VERSION)
# =====================================================================================

import io
import os
import zipfile
import requests
from datetime import datetime
from functools import lru_cache
from typing import Optional, List, Tuple
import pandas as pd
import streamlit as st

# ==== Configuration IBM Watsonx ====
# √Ä renseigner dans st.secrets ou en variables d'environnement
try:
    IBM_CLOUD_API_KEY = st.secrets["IBM_CLOUD_API_KEY"]
    IBM_WATSONX_PROJECT_ID = st.secrets["IBM_WATSONX_PROJECT_ID"]
    IBM_WATSONX_URL = st.secrets.get("IBM_WATSONX_URL", "https://us-south.ml.cloud.ibm.com")
    WATSONX_AVAILABLE = True
except KeyError:
    IBM_CLOUD_API_KEY = None
    IBM_WATSONX_PROJECT_ID = None
    IBM_WATSONX_URL = "https://us-south.ml.cloud.ibm.com"
    WATSONX_AVAILABLE = False

# ==== Langchain utils (RAG optionnel) ====
try:
    from langchain_openai import OpenAIEmbeddings
    from langchain_community.vectorstores import FAISS
    from langchain_text_splitters import RecursiveCharacterTextSplitter
    from langchain_core.documents import Document as LCDocument
    from langchain_core.prompts import PromptTemplate
    LANGCHAIN_AVAILABLE = True
except ImportError:
    OpenAIEmbeddings = None
    FAISS = None
    LANGCHAIN_AVAILABLE = False

# ===================== IBM WATSONX CLIENT =====================

class WatsonxLLMClient:
    """
    Client pour IBM Watsonx AI
    Compatible avec l'interface invoke() utilis√©e dans le code original
    """
    
    def __init__(
        self,
        api_key: str,
        project_id: str,
        url: str = "https://us-south.ml.cloud.ibm.com",
        model_id: str = "ibm/granite-13b-chat-v2",
        max_tokens: int = 4000,
        temperature: float = 0.2
    ):
        self.api_key = api_key
        self.project_id = project_id
        self.url = url
        self.model_id = model_id
        self.max_tokens = max_tokens
        self.temperature = temperature
        self.access_token = None
        self.token_expiry = None
        
    def _get_iam_token(self) -> str:
        """
        G√©n√®re un token IAM IBM Cloud √† partir de la cl√© API
        Le token est valide 1h, on le cache pour √©viter les appels r√©p√©t√©s
        """
        import time
        
        # V√©rifier si le token est encore valide (marge de 5 min)
        if self.access_token and self.token_expiry:
            if time.time() < (self.token_expiry - 300):
                return self.access_token
        
        # G√©n√©rer un nouveau token
        iam_url = "https://iam.cloud.ibm.com/identity/token"
        headers = {
            "Content-Type": "application/x-www-form-urlencoded"
        }
        data = {
            "grant_type": "urn:ibm:params:oauth:grant-type:apikey",
            "apikey": self.api_key
        }
        
        try:
            response = requests.post(iam_url, headers=headers, data=data, timeout=10)
            response.raise_for_status()
            token_data = response.json()
            
            self.access_token = token_data["access_token"]
            # Token valide 3600 secondes (1h)
            self.token_expiry = time.time() + token_data.get("expires_in", 3600)
            
            return self.access_token
            
        except Exception as e:
            raise Exception(f"‚ùå Erreur g√©n√©ration token IAM: {e}")
    
    def invoke(self, prompt: str) -> 'WatsonxResponse':
        """
        M√©thode compatible avec l'interface Langchain invoke()
        Envoie une requ√™te √† Watsonx et retourne une r√©ponse
        """
        token = self._get_iam_token()
        
        # URL de l'API Watsonx
        endpoint = f"{self.url}/ml/v1/text/generation?version=2023-05-29"
        
        # Headers
        headers = {
            "Authorization": f"Bearer {token}",
            "Content-Type": "application/json",
            "Accept": "application/json"
        }
        
        # Payload Watsonx
        payload = {
            "model_id": self.model_id,
            "input": prompt,
            "parameters": {
                "decoding_method": "greedy",
                "max_new_tokens": self.max_tokens,
                "temperature": self.temperature,
                "repetition_penalty": 1.1,
                "stop_sequences": []
            },
            "project_id": self.project_id
        }
        
        try:
            response = requests.post(
                endpoint,
                headers=headers,
                json=payload,
                timeout=120  # 2 minutes timeout
            )
            response.raise_for_status()
            result = response.json()
            
            # Extraction du texte g√©n√©r√©
            generated_text = result.get("results", [{}])[0].get("generated_text", "")
            
            return WatsonxResponse(generated_text)
            
        except requests.exceptions.Timeout:
            raise Exception("‚è±Ô∏è Timeout: Watsonx n'a pas r√©pondu dans les d√©lais")
        except requests.exceptions.HTTPError as e:
            error_detail = e.response.text if hasattr(e.response, 'text') else str(e)
            raise Exception(f"‚ùå Erreur HTTP Watsonx ({e.response.status_code}): {error_detail}")
        except Exception as e:
            raise Exception(f"‚ùå Erreur Watsonx: {e}")


class WatsonxResponse:
    """
    Classe de r√©ponse compatible avec l'interface Langchain
    """
    def __init__(self, content: str):
        self.content = content
    
    def __str__(self):
        return self.content


# ===================== CONFIGURATION STREAMLIT =====================
st.set_page_config(
    page_title="Assistant AI Mainframe Pro",
    page_icon="ü§ñ",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ===================== SESSION STATE INITIALIZATION =====================
if 'rgc_analysis_result' not in st.session_state:
    st.session_state.rgc_analysis_result = None
if 'rgc_df_config' not in st.session_state:
    st.session_state.rgc_df_config = None
if 'rgc_resume_raw' not in st.session_state:
    st.session_state.rgc_resume_raw = None
if 'rgc_stats_dict' not in st.session_state:
    st.session_state.rgc_stats_dict = None
if 'rgc_uploaded_filename' not in st.session_state:
    st.session_state.rgc_uploaded_filename = None
if 'show_advanced_analysis' not in st.session_state:
    st.session_state.show_advanced_analysis = False

# ===================== CUSTOM CSS PRO (identique) =====================
st.markdown("""
<style>
    /* ===== THEME PRINCIPAL ===== */
    :root {
        --primary-color: #4EA3FF;
        --secondary-color: #0066CC;
        --accent-color: #00D4FF;
        --bg-dark: #0E1117;
        --bg-card: #1E1E2E;
        --text-primary: #FFFFFF;
        --text-secondary: #B8B8C8;
        --success: #00D9A5;
        --warning: #FFB020;
        --error: #FF4757;
    }
    
    /* ===== BACKGROUND ===== */
    .stApp {
        background: linear-gradient(135deg, #0E1117 0%, #1a1d29 100%);
    }
    
    /* ===== HEADER PRO ===== */
    .ai-header {
        background: linear-gradient(135deg, #1E1E2E 0%, #2B2D3E 100%);
        padding: 2rem;
        border-radius: 16px;
        text-align: center;
        margin-bottom: 2rem;
        box-shadow: 0 8px 32px rgba(78, 163, 255, 0.15);
        border: 1px solid rgba(78, 163, 255, 0.2);
        position: relative;
        overflow: hidden;
    }
    
    .ai-header::before {
        content: '';
        position: absolute;
        top: 0;
        left: -100%;
        width: 100%;
        height: 100%;
        background: linear-gradient(90deg, transparent, rgba(78, 163, 255, 0.1), transparent);
        animation: shine 3s infinite;
    }
    
    @keyframes shine {
        0% { left: -100%; }
        100% { left: 100%; }
    }
    
    .ai-header h1 {
        font-size: 2.5rem;
        font-weight: 800;
        margin: 0;
        background: linear-gradient(135deg, #4EA3FF 0%, #00D4FF 100%);
        -webkit-background-clip: text;
        -webkit-text-fill-color: transparent;
        background-clip: text;
        position: relative;
        z-index: 1;
    }
    
    .ai-header .subtitle {
        color: var(--text-secondary);
        font-size: 1rem;
        margin-top: 0.5rem;
        font-weight: 300;
        letter-spacing: 1px;
    }
    
    /* ===== CARDS GLASSMORPHISM ===== */
    .glass-card {
        background: rgba(30, 30, 46, 0.7);
        backdrop-filter: blur(10px);
        border-radius: 12px;
        padding: 1.5rem;
        border: 1px solid rgba(78, 163, 255, 0.15);
        box-shadow: 0 8px 32px rgba(0, 0, 0, 0.3);
        margin-bottom: 1.5rem;
        transition: all 0.3s ease;
    }
    
    .glass-card:hover {
        border-color: rgba(78, 163, 255, 0.4);
        box-shadow: 0 12px 40px rgba(78, 163, 255, 0.2);
        transform: translateY(-2px);
    }
    
    /* ===== INFO BOXES ===== */
    .info-box {
        background: linear-gradient(135deg, rgba(78, 163, 255, 0.1) 0%, rgba(0, 212, 255, 0.1) 100%);
        border-left: 4px solid var(--primary-color);
        padding: 1rem 1.5rem;
        border-radius: 8px;
        margin: 1rem 0;
        color: var(--text-primary);
        font-size: 0.95rem;
        line-height: 1.6;
    }
    
    .success-box {
        background: linear-gradient(135deg, rgba(0, 217, 165, 0.1) 0%, rgba(0, 255, 200, 0.1) 100%);
        border-left-color: var(--success);
    }
    
    .warning-box {
        background: linear-gradient(135deg, rgba(255, 176, 32, 0.1) 0%, rgba(255, 200, 100, 0.1) 100%);
        border-left-color: var(--warning);
    }
    
    .error-box {
        background: linear-gradient(135deg, rgba(255, 71, 87, 0.1) 0%, rgba(255, 100, 120, 0.1) 100%);
        border-left-color: var(--error);
    }
    
    /* ===== BUTTONS PRO ===== */
    .stButton>button {
        background: linear-gradient(135deg, var(--primary-color) 0%, var(--secondary-color) 100%);
        color: white;
        font-weight: 600;
        border-radius: 10px;
        border: none;
        padding: 0.75rem 2rem;
        font-size: 1rem;
        transition: all 0.3s ease;
        box-shadow: 0 4px 15px rgba(78, 163, 255, 0.3);
        text-transform: uppercase;
        letter-spacing: 0.5px;
    }
    
    .stButton>button:hover {
        background: linear-gradient(135deg, var(--secondary-color) 0%, var(--primary-color) 100%);
        box-shadow: 0 6px 25px rgba(78, 163, 255, 0.5);
        transform: translateY(-2px);
    }
    
    .stButton>button:active {
        transform: translateY(0);
    }
    
    .stButton>button:disabled {
        background: linear-gradient(135deg, #2a2a3a 0%, #3a3a4a 100%);
        box-shadow: none;
        opacity: 0.5;
    }
    
    /* ===== SIDEBAR PRO ===== */
    [data-testid="stSidebar"] {
        background: linear-gradient(180deg, #1a1d29 0%, #0E1117 100%);
        border-right: 1px solid rgba(78, 163, 255, 0.15);
    }
    
    [data-testid="stSidebar"] .stRadio > label {
        background: rgba(78, 163, 255, 0.05);
        padding: 0.5rem 1rem;
        border-radius: 8px;
        margin: 0.25rem 0;
        transition: all 0.2s ease;
    }
    
    [data-testid="stSidebar"] .stRadio > label:hover {
        background: rgba(78, 163, 255, 0.15);
    }
    
    /* ===== INPUTS ===== */
    .stTextInput input, .stTextArea textarea {
        background: rgba(30, 30, 46, 0.6) !important;
        border: 1px solid rgba(78, 163, 255, 0.3) !important;
        border-radius: 8px !important;
        color: var(--text-primary) !important;
        padding: 0.75rem !important;
        transition: all 0.3s ease !important;
    }
    
    .stTextInput input:focus, .stTextArea textarea:focus {
        border-color: var(--primary-color) !important;
        box-shadow: 0 0 0 2px rgba(78, 163, 255, 0.2) !important;
    }
    
    /* ===== FILE UPLOADER ===== */
    [data-testid="stFileUploader"] {
        background: rgba(30, 30, 46, 0.4);
        border: 2px dashed rgba(78, 163, 255, 0.3);
        border-radius: 12px;
        padding: 2rem;
        transition: all 0.3s ease;
    }
    
    [data-testid="stFileUploader"]:hover {
        border-color: var(--primary-color);
        background: rgba(78, 163, 255, 0.05);
    }
    
    /* ===== DATAFRAME ===== */
    .stDataFrame {
        border-radius: 12px;
        overflow: hidden;
        box-shadow: 0 4px 20px rgba(0, 0, 0, 0.3);
    }
    
    /* ===== CODE BLOCKS ===== */
    .stCodeBlock {
        border-radius: 12px;
        border: 1px solid rgba(78, 163, 255, 0.2);
        box-shadow: 0 4px 15px rgba(0, 0, 0, 0.3);
    }
    
    /* ===== SPINNER ===== */
    .stSpinner > div {
        border-top-color: var(--primary-color) !important;
    }
    
    /* ===== TABS ===== */
    .stTabs [data-baseweb="tab-list"] {
        gap: 8px;
        background: rgba(30, 30, 46, 0.5);
        padding: 0.5rem;
        border-radius: 12px;
    }
    
    .stTabs [data-baseweb="tab"] {
        background: transparent;
        border-radius: 8px;
        color: var(--text-secondary);
        padding: 0.75rem 1.5rem;
        transition: all 0.3s ease;
    }
    
    .stTabs [aria-selected="true"] {
        background: linear-gradient(135deg, var(--primary-color) 0%, var(--secondary-color) 100%);
        color: white;
    }
    
    /* ===== FOOTER PRO ===== */
    .footer-pro {
        margin-top: 4rem;
        padding: 2rem;
        border-radius: 16px;
        text-align: center;
        background: linear-gradient(135deg, #1E1E2E 0%, #2B2D3E 100%);
        border: 1px solid rgba(78, 163, 255, 0.2);
        box-shadow: 0 8px 32px rgba(0, 0, 0, 0.3);
    }
    
    .footer-title {
        color: var(--primary-color);
        font-weight: 700;
        font-size: 1.1rem;
        margin-bottom: 1rem;
    }
    
    .footer-team {
        color: var(--text-secondary);
        font-size: 0.9rem;
        line-height: 1.8;
    }
    
    .footer-team span {
        color: var(--accent-color);
        font-weight: 600;
    }
    
    /* ===== ANIMATIONS ===== */
    @keyframes fadeIn {
        from { opacity: 0; transform: translateY(20px); }
        to { opacity: 1; transform: translateY(0); }
    }
    
    .glass-card, .info-box {
        animation: fadeIn 0.5s ease-out;
    }
    
    /* ===== RESPONSIVE ===== */
    @media (max-width: 768px) {
        .ai-header h1 {
            font-size: 1.8rem;
        }
        
        .glass-card {
            padding: 1rem;
        }
    }
</style>
""", unsafe_allow_html=True)

# ===================== HEADER PRO =====================
st.markdown("""
<div class="ai-header">
    <h1>ü§ñ Assistant AI Mainframe</h1>
    <div class="subtitle">IBM A&T - Powered by IBM Watsonx</div>
</div>
""", unsafe_allow_html=True)

# ===================== V√âRIFICATION CONFIG =====================
if not WATSONX_AVAILABLE or not IBM_CLOUD_API_KEY or not IBM_WATSONX_PROJECT_ID:
    st.markdown("""
    <div class="error-box">
        ‚ö†Ô∏è <strong>Configuration IBM Watsonx manquante</strong><br><br>
        Ajoutez dans <code>.streamlit/secrets.toml</code> :<br>
        <pre>
IBM_CLOUD_API_KEY = "votre_api_key_ibm"
IBM_WATSONX_PROJECT_ID = "votre_project_id"
IBM_WATSONX_URL = "https://us-south.ml.cloud.ibm.com"  # Optionnel
        </pre>
        <br>
        üìö Documentation : <a href="https://cloud.ibm.com/apidocs/watsonx-ai" target="_blank">IBM Watsonx AI API</a>
    </div>
    """, unsafe_allow_html=True)
    st.stop()

# ===================== LANGUE =====================
lang_choice = st.sidebar.selectbox(
    "üåê Language / Langue", 
    ["Fran√ßais", "English"], 
    index=0,
    help="S√©lectionnez votre langue pr√©f√©r√©e"
)
LANG_FR = (lang_choice == "Fran√ßais")

def T(fr: str, en: str) -> str:
    return fr if LANG_FR else en

# ===================== PROMPT ENGINE =====================
import yaml

@lru_cache(maxsize=1)
def load_prompt_engine(path: str = "PromptEngine.yaml") -> dict:
    try:
        if os.path.exists(path):
            with open(path, "r", encoding="utf-8") as f:
                return yaml.safe_load(f) or {}
        else:
            st.markdown(f"""
            <div class="warning-box">
                ‚ÑπÔ∏è Fichier <code>{path}</code> introuvable. Utilisation des prompts par d√©faut.
            </div>
            """, unsafe_allow_html=True)
            return {}
    except Exception as e:
        st.markdown(f"""
        <div class="error-box">
            ‚ùå Erreur lors du chargement de <code>{path}</code>: {e}
        </div>
        """, unsafe_allow_html=True)
        return {}

def get_prompt(section: str, lang_key: Optional[str] = None, **kwargs) -> str:
    pe = load_prompt_engine()
    defaults = pe.get("defaults", {})
    invalid_message = defaults.get("invalid_message", "‚ùå Code invalide")
    no_jcl = defaults.get("no_jcl_in_test_mode", "Ne pas g√©n√©rer de JCL")

    if section == "ANALYSE_DOC":
        node = pe.get("ANALYSE_DOC", {}).get("GENERIQUE", "")
    elif lang_key:
        node = pe.get(section, {}).get(lang_key.upper(), "")
    else:
        node = ""

    if not node:
        if section == "ANALYSE_DOC":
            node = "R√¥le: Analyste documentaire.\n{document_content}"
        elif section == "JCL_GENERATION":
            node = "R√¥le: Expert JCL {lang}.\nCode:\n{source_code}"
        else:
            node = "R√¥le: QA Test {lang}.\nModule:{module_base}\nCode:\n{source_code}"

    try:
        return node.format(
            invalid_message=invalid_message,
            no_jcl_in_test_mode=no_jcl,
            **kwargs
        )
    except KeyError as e:
        st.error(f"‚ùå Param√®tre manquant: {e}")
        return node

# ===================== WATSONX CLIENT WRAPPER =====================
def llm_client(max_tokens: int = 4000, temperature: float = 0.2):
    """
    Factory pour cr√©er un client Watsonx
    Compatible avec l'interface du code original
    """
    if not WATSONX_AVAILABLE or not IBM_CLOUD_API_KEY or not IBM_WATSONX_PROJECT_ID:
        return None
    
    try:
        # Mod√®les disponibles Watsonx :
        # - ibm/granite-13b-chat-v2 (recommand√© pour le chat)
        # - ibm/granite-13b-instruct-v2
        # - meta-llama/llama-2-70b-chat
        # - mistralai/mixtral-8x7b-instruct-v01
        
        return WatsonxLLMClient(
            api_key=IBM_CLOUD_API_KEY,
            project_id=IBM_WATSONX_PROJECT_ID,
            url=IBM_WATSONX_URL,
            model_id="ibm/granite-13b-chat-v2",  # Mod√®le par d√©faut
            max_tokens=max_tokens,
            temperature=temperature
        )
    except Exception as e:
        st.error(f"‚ùå Erreur client Watsonx: {e}")
        return None

# ===================== HELPERS (identiques) =====================
def _normalize_text(s: str) -> str:
    return " ".join(s.replace("\r", "\n").split())

def read_pdf_bytes(data: bytes) -> str:
    try:
        import PyPDF2
        text = []
        with io.BytesIO(data) as fh:
            reader = PyPDF2.PdfReader(fh)
            for page in reader.pages:
                extracted = page.extract_text()
                if extracted:
                    text.append(extracted)
        result = _normalize_text("\n".join(text))
        if result.strip():
            return result
    except:
        pass
    
    try:
        import pdfplumber
        text = []
        with pdfplumber.open(io.BytesIO(data)) as pdf:
            for page in pdf.pages:
                extracted = page.extract_text()
                if extracted:
                    text.append(extracted)
        return _normalize_text("\n".join(text))
    except:
        return ""

def read_docx_bytes(data: bytes) -> str:
    try:
        import docx
        with io.BytesIO(data) as fh:
            doc = docx.Document(fh)
            return _normalize_text("\n".join(p.text for p in doc.paragraphs))
    except:
        return ""

def read_txt_bytes(data: bytes, encoding: str = "utf-8") -> str:
    try:
        return _normalize_text(data.decode(encoding, errors="ignore"))
    except:
        return ""

def extract_document_content(uploaded_file) -> str:
    if not uploaded_file:
        return ""
    
    name = (uploaded_file.name or "").lower()
    
    try:
        raw = uploaded_file.read()
    except Exception as e:
        st.error(f"‚ùå Erreur de lecture: {e}")
        return ""

    def safe_truncate(text: str, max_chars: int = 150_000) -> str:
        if len(text) > max_chars:
            st.markdown(f"""
            <div class="warning-box">
                ‚ö†Ô∏è Contenu tronqu√© √† {max_chars:,} caract√®res
            </div>
            """, unsafe_allow_html=True)
        return text[:max_chars]

    if name.endswith(".pdf"):
        return safe_truncate(read_pdf_bytes(raw))
    if name.endswith(".docx"):
        return safe_truncate(read_docx_bytes(raw))
    if name.endswith(".txt"):
        return safe_truncate(read_txt_bytes(raw))

    if name.endswith(".zip"):
        texts = []
        try:
            with zipfile.ZipFile(io.BytesIO(raw)) as z:
                for info in z.infolist():
                    n = info.filename.lower()
                    if n.endswith((".pdf", ".docx", ".txt")) and not info.is_dir():
                        try:
                            data = z.read(info)
                            if n.endswith(".pdf"):
                                texts.append(read_pdf_bytes(data))
                            elif n.endswith(".docx"):
                                texts.append(read_docx_bytes(data))
                            elif n.endswith(".txt"):
                                texts.append(read_txt_bytes(data))
                            
                            if sum(len(t) for t in texts) > 200_000:
                                break
                        except Exception as e:
                            st.warning(f"‚ö†Ô∏è Impossible de lire {info.filename}")
                            continue
        except zipfile.BadZipFile:
            st.markdown("""
            <div class="error-box">
                ‚ùå Fichier ZIP invalide ou corrompu
            </div>
            """, unsafe_allow_html=True)
            return ""
        
        return safe_truncate("\n\n---\n\n".join(t for t in texts if t.strip()))

    return ""

def detect_critical_conflicts(df: pd.DataFrame) -> pd.DataFrame:
    """
    Post-traitement pour garantir la d√©tection des conflits critiques
    m√™me si le LLM les a manqu√©s.
    """
    if df.empty or 'Nom Programme' not in df.columns:
        return df
    
    required_cols = ['Nom Programme', 'Environnement', 'Couloir', 'Niveau de Risque']
    if not all(col in df.columns for col in required_cols):
        return df
    
    df['_env_couloir_key'] = df['Environnement'].astype(str) + '_' + df['Couloir'].astype(str)
    
    for prog_name, group in df.groupby('Nom Programme'):
        if len(group) > 1:
            duplicates = group[group.duplicated(subset=['_env_couloir_key'], keep=False)]
            
            if not duplicates.empty:
                indices = duplicates.index
                for idx in indices:
                    df.at[idx, 'Niveau de Risque'] = 'üö® Conflit de duplication dans le m√™me environnement'
            else:
                unique_envs = group['Environnement'].nunique()
                unique_couloirs = group['Couloir'].nunique()
                
                if unique_envs > 1:
                    for idx in group.index:
                        current_risk = df.at[idx, 'Niveau de Risque']
                        if 'üö®' not in str(current_risk):
                            df.at[idx, 'Niveau de Risque'] = '‚ö†Ô∏è Programme dupliqu√© (multi-environnements)'
                elif unique_couloirs > 1:
                    for idx in group.index:
                        current_risk = df.at[idx, 'Niveau de Risque']
                        if 'üö®' not in str(current_risk):
                            df.at[idx, 'Niveau de Risque'] = '‚ö†Ô∏è Risque de modification (multi-couloir)'
    
    df.drop(columns=['_env_couloir_key'], inplace=True)
    
    return df

# ===================== UI LABELS =====================
TEXTS = {
    "Fran√ßais": {
        "choose_mode": "‚öôÔ∏è Mode de traitement",
        "modes": [
            "üìÑ Analyse documentaire", 
            "üîß G√©n√©ration JCL", 
            "üß™ Test COBOL",
            "‚öôÔ∏è Analyse RGC"
        ],
    },
    "English": {
        "choose_mode": "‚öôÔ∏è Processing Mode",
        "modes": [
            "üìÑ Document Analysis", 
            "üîß JCL Generation", 
            "üß™ COBOL Testing",
            "‚öôÔ∏è RGC Analysis"
        ],
    }
}
TXT = TEXTS["Fran√ßais"] if LANG_FR else TEXTS["English"]

# ===================== MODE SELECTOR =====================
st.sidebar.markdown("---")
mode = st.sidebar.radio(
    TXT["choose_mode"], 
    TXT["modes"], 
    index=0,
    help=T("S√©lectionnez le mode de traitement souhait√©", "Select your processing mode")
)

# ===================== MODE 1 : ANALYSE DOC (code identique sauf le client) =====================
if mode == TXT["modes"][0]:
    st.markdown('<div class="glass-card">', unsafe_allow_html=True)
    st.header(T("üìÑ Analyse Documentaire Intelligente", "üìÑ Intelligent Document Analysis"))
    st.markdown('</div>', unsafe_allow_html=True)

    col1, col2 = st.columns(2)
    with col1:
        uploaded_zip = st.file_uploader(
            "üì¶ " + T("Fichier ZIP", "ZIP File"),
            type=["zip"],
            help=T("Charger un fichier ZIP contenant des documents", "Upload a ZIP file with documents")
        )
    with col2:
        uploaded_pdfs = st.file_uploader(
            "üìÑ " + T("Fichiers PDF", "PDF Files"),
            type=["pdf"],
            accept_multiple_files=True,
            help=T("Charger un ou plusieurs PDFs", "Upload one or more PDFs")
        )

    pdf_buffers: List[Tuple[str, bytes]] = []
    
    if uploaded_zip or uploaded_pdfs:
        with st.spinner(T("üìö Traitement des documents...", "üìö Processing documents...")):
            if uploaded_zip:
                try:
                    with zipfile.ZipFile(uploaded_zip, "r") as zip_ref:
                        for name in zip_ref.namelist():
                            if name.lower().endswith(".pdf") and not name.startswith("__MACOSX"):
                                try:
                                    with zip_ref.open(name) as pdf_file:
                                        pdf_buffers.append((name, pdf_file.read()))
                                except Exception as e:
                                    st.warning(f"‚ö†Ô∏è {name}: {e}")
                except zipfile.BadZipFile:
                    st.markdown('<div class="error-box">‚ùå ZIP invalide</div>', unsafe_allow_html=True)
            
            if uploaded_pdfs:
                for f in uploaded_pdfs:
                    try:
                        pdf_buffers.append((f.name, f.read()))
                    except Exception as e:
                        st.warning(f"‚ö†Ô∏è {f.name}: {e}")
    
    if pdf_buffers:
        st.markdown(f"""
        <div class="success-box">
            ‚úÖ <strong>{len(pdf_buffers)} document(s) charg√©(s)</strong>
        </div>
        """, unsafe_allow_html=True)

    st.markdown('<div class="glass-card">', unsafe_allow_html=True)
    context_mode = st.radio(
        "üéØ " + T("Contexte d'analyse", "Analysis Context"),
        [T("Analyse G√©n√©rique", "Generic Analysis"),
         T("Analyse CV / Profil", "CV / Profile Analysis")],
        horizontal=True
    )

    question = st.text_area(
        "üí¨ " + T("Votre question", "Your question"),
        placeholder=T(
            "Ex: Quelles sont les comp√©tences principales ?",
            "Ex: What are the main skills?"
        ),
        height=100
    )
    st.markdown('</div>', unsafe_allow_html=True)

    if st.button(
        "üöÄ " + T("ANALYSER", "ANALYZE"), 
        disabled=not question or not pdf_buffers,
        use_container_width=True
    ):
        if not pdf_buffers:
            st.markdown('<div class="error-box">‚ùå Aucun document charg√©</div>', unsafe_allow_html=True)
        else:
            texts = []
            for name, raw in pdf_buffers:
                extracted = read_pdf_bytes(raw)
                if extracted.strip():
                    texts.append(f"=== {name} ===\n{extracted}")
            
            document_content = "\n\n---\n\n".join(texts)[:150_000]

            if not document_content.strip():
                st.markdown('<div class="error-box">‚ùå Aucun texte extrait</div>', unsafe_allow_html=True)
            else:
                if "CV" in context_mode or "Profile" in context_mode:
                    prompt_text = f"""
Tu es une IA experte en analyse de CV et matching de profils.

**Mission :**
- Analyse approfondie des documents fournis
- Identification des comp√©tences (explicites et implicites)
- √âvaluation compl√®te avant r√©ponse
- Aucun d√©tail important omis

**Format de r√©ponse :**
1. R√©ponse directe
2. D√©tails et justifications
3. Score (si applicable) : X/100
4. Recommandations

**Question :** {question}

**Documents :**
{document_content}
"""
                else:
                    prompt_text = f"""
R√¥le : Vous √™tes un Analyste Strat√©gique Expert, sp√©cialis√© dans l'interpr√©tation, l'analyse et la projection d'informations bas√©es sur des documents.

Objectif Principal : R√©pondre aux questions en utilisant le contenu des documents fournis, mais en allant au-del√† de la simple extraction. Vous devez analyser, synth√©tiser et g√©n√©rer des r√©ponses d√©ductives pour √©clairer l'utilisateur sur des cons√©quences ou des sc√©narios non explicitement mentionn√©s.

Instructions de Traitement et d'Intelligence
Analyse Compl√®te et D√©duction : Lisez tous les documents. Vous devez non seulement extraire les faits, mais aussi d√©duire, interpr√©ter et extrapoler des conclusions logiques et plausibles bas√©es sur ces faits.

G√©n√©ration de Contenu : Vous √™tes autoris√© √† inventer des r√©ponses (dans le sens de cr√©er des analyses, des rapports, des projections ou des sc√©narios) √† condition qu'elles soient directement et logiquement justifi√©es par les donn√©es pr√©sentes dans les documents fournis.

Synth√®se Strat√©gique : Si la question le demande (ex: "Quel impact cela pourrait-il avoir ?", "Quelle est la tendance ?"), combinez les informations de mani√®re cr√©ative pour g√©n√©rer une r√©ponse strat√©gique.

Capacit√©s Avanc√©es : Utilisez des calculs (projections, tendances, pourcentages) et des comparaisons avanc√©es pour √©tayer vos d√©ductions.

Contraintes de R√©ponse
Lien avec le Contexte : Toute d√©duction ou invention doit rester √©troitement li√©e au contexte des documents. Si vous cr√©ez une nouvelle information, elle doit √™tre une cons√©quence logique et clairement √©tay√©e par le texte source.

Absence d'Information Justificative : Si la question n√©cessite une d√©duction ou une invention qui ne peut pas √™tre logiquement justifi√©e par les documents, vous devez r√©pondre strictement par la phrase suivante, et rien d'autre :

"Information ou d√©duction non justifi√©e par les documents analys√©s."

Format d'Analyse : Structurez vos r√©ponses de mani√®re claire, concise et professionnelle :

Utilisez des listes num√©rot√©es ou √† puces pour les extractions de faits.

S√©parez clairement les Faits Extraits des D√©ductions/Sc√©narios Propos√©s.

Mettez en gras les faits critiques et les conclusions d√©ductives.

CONTENU CV :
{document_content}
"""

                client = llm_client(max_tokens=3000, temperature=0.2)
                if not client:
                    st.markdown('<div class="error-box">‚ùå Client Watsonx indisponible</div>', unsafe_allow_html=True)
                else:
                    with st.spinner(T("üß† Analyse en cours...", "üß† Analyzing...")):
                        try:
                            response = client.invoke(prompt_text)
                            result = response.content if hasattr(response, 'content') else str(response)
                        except Exception as e:
                            st.error(f"‚ùå Erreur Watsonx: {e}")
                            result = None

                    if result:
                        st.markdown('<div class="glass-card">', unsafe_allow_html=True)
                        st.subheader("üß† " + T("R√©ponse de l'IA", "AI Answer"))
                        st.markdown(result)
                        st.markdown('</div>', unsafe_allow_html=True)

                        try:
                            from docx import Document
                            doc = Document()
                            doc.add_heading(T("Rapport d'Analyse IA", "AI Analysis Report"), 0)
                            doc.add_heading(T("Question", "Question"), 1)
                            doc.add_paragraph(question)
                            doc.add_heading(T("R√©ponse", "Answer"), 1)
                            doc.add_paragraph(result)
                            
                            buf = io.BytesIO()
                            doc.save(buf)
                            buf.seek(0)
                            
                            st.download_button(
                                "üì• " + T("T√©l√©charger Rapport (Word)", "Download Report (Word)"),
                                data=buf,
                                file_name=T("rapport_analyse.docx", "analysis_report.docx"),
                                mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document",
                                use_container_width=True
                            )
                        except ImportError:
                            st.download_button(
                                "üì• " + T("T√©l√©charger Rapport (TXT)", "Download Report (TXT)"),
                                data=result.encode("utf-8"),
                                file_name=T("rapport_analyse.txt", "analysis_report.txt"),
                                use_container_width=True
                            )

# ===================== MODE 2 : JCL GENERATION (identique) =====================
# [Le code reste identique, seul le client LLM change]
elif mode == TXT["modes"][1]:
    # [Code identique au pr√©c√©dent - je l'omets pour la bri√®vet√©]
    # Utilisez simplement llm_client() qui retourne maintenant un WatsonxLLMClient
    pass

# ===================== MODE 3 : COBOL TEST (identique) =====================
# [Le code reste identique, seul le client LLM change]
elif mode == TXT["modes"][2]:
    # [Code identique au pr√©c√©dent]
    pass

# ===================== MODE 4 : RGC ANALYSIS (identique) =====================
# [Le code reste identique, seul le client LLM change]
elif mode == TXT["modes"][3]:
    # [Code identique au pr√©c√©dent]
    pass

# ===================== FOOTER PRO =====================
st.markdown("""
<div class="footer-pro">
    <div class="footer-title">üíº Modernisation Mainframe IBM A&T</div>
    <div class="footer-team">
        üë• <span>√âquipe</span> : Youness ‚Ä¢ Hanane ‚Ä¢ Nezha ‚Ä¢ Aimane ‚Ä¢ Khaoula ‚Ä¢ Naoufal ‚Ä¢ Imane ‚Ä¢ Meriem
    </div>
    <div style="margin-top: 1rem; color: #666; font-size: 0.85rem;">
        Powered by <strong>IBM Watsonx</strong> ‚Ä¢ ¬© 2025 Tous droits r√©serv√©s
    </div>
</div>
""", unsafe_allow_html=True)